{
    "project name": "apache_mrunit",
    "vulnerable dependencies": {
        "org.apache.hadoop:hadoop-common:2.6.0": {
            "used-method num": 144,
            "used method": [
                "org.apache.hadoop.conf.Configuration:java.util.Iterator iterator()",
                "org.apache.hadoop.util.ShutdownHookManager$HookEntry:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder:org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto$Builder clone()",
                "org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.util.ReflectionUtils:java.lang.Object newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.conf.Configuration:void setIfUnset(java.lang.String,java.lang.String)",
                "org.apache.hadoop.metrics2.impl.MetricsConfig$1:java.util.Iterator iterator()",
                "org.apache.hadoop.metrics2.impl.MsInfo:java.lang.String toString()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:int compareTo(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:int hashCode()",
                "org.apache.hadoop.fs.FileSystem:void <clinit>()",
                "org.apache.hadoop.ipc.Client$ConnectionId:int hashCode()",
                "org.apache.hadoop.conf.Configuration:java.lang.String get(java.lang.String)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto:int hashCode()",
                "org.apache.hadoop.fs.permission.AclStatus:int hashCode()",
                "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.FileStatus:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.metrics2.util.Quantile:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.LocatedFileStatus:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:int hashCode()",
                "org.apache.hadoop.io.UTF8:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.security.UserGroupInformation:int hashCode()",
                "org.apache.hadoop.io.UTF8:int compareTo(java.lang.Object)",
                "org.apache.hadoop.io.DataInputBuffer:void reset(byte[],int)",
                "org.apache.hadoop.io.WritableComparator:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.io.Text:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:int hashCode()",
                "org.apache.hadoop.io.Text:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder:org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$Builder clone()",
                "org.apache.hadoop.fs.Path:int compareTo(java.lang.Object)",
                "org.apache.hadoop.fs.FileSystem:org.apache.hadoop.fs.LocalFileSystem getLocal(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:int hashCode()",
                "org.apache.hadoop.fs.FileSystem$Cache$Key:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder:org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto$Builder clone()",
                "org.apache.hadoop.util.RunJar:void unJar(java.io.File,java.io.File)",
                "org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:int hashCode()",
                "org.apache.hadoop.io.serializer.SerializationFactory:org.apache.hadoop.io.serializer.Deserializer getDeserializer(java.lang.Class)",
                "org.apache.hadoop.ipc.Client$ConnectionId:boolean equals(java.lang.Object)",
                "org.apache.hadoop.util.ReflectionUtils:void setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.io.NullWritable:int compareTo(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth:int hashCode()",
                "org.apache.hadoop.metrics2.impl.MetricsBuffer:java.util.Iterator iterator()",
                "org.apache.hadoop.fs.Path:java.lang.String toString()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder:org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$Builder clone()",
                "org.apache.hadoop.conf.Configuration$IntegerRanges:java.util.Iterator iterator()",
                "org.apache.hadoop.io.SequenceFile$Metadata:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.BinaryComparable:int compareTo(java.lang.Object)",
                "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:java.lang.Integer next()",
                "org.apache.hadoop.io.serializer.SerializationFactory:org.apache.hadoop.io.serializer.Serializer getSerializer(java.lang.Class)",
                "org.apache.hadoop.util.ReflectionUtils:void <clinit>()",
                "org.apache.hadoop.io.DataOutputBuffer:void <init>()",
                "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.LocatedFileStatus:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.DataOutputBuffer:byte[] getData()",
                "org.apache.hadoop.metrics2.util.Quantile:int compareTo(java.lang.Object)",
                "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:org.apache.hadoop.fs.Path next()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto$Builder:org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsResponseProto$Builder clone()",
                "org.apache.hadoop.io.DataOutputBuffer:int getLength()",
                "org.apache.hadoop.fs.permission.FsPermission:int hashCode()",
                "org.apache.hadoop.fs.FileUtil:void unTar(java.io.File,java.io.File)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder:org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth$Builder clone()",
                "org.apache.hadoop.fs.permission.FsPermission:boolean equals(java.lang.Object)",
                "org.apache.hadoop.security.UserGroupInformation$RealUser:int hashCode()",
                "org.apache.hadoop.security.User:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto:int hashCode()",
                "org.apache.hadoop.io.LongWritable:int compareTo(java.lang.Object)",
                "org.apache.hadoop.crypto.CipherSuite:java.lang.String toString()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto$Builder:org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto$Builder clone()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.IntWritable:int compareTo(java.lang.Object)",
                "org.apache.hadoop.conf.Configuration:void <init>(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.fs.permission.AclEntry:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.FileSystem$Cache$Key:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto$Builder:org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto$Builder clone()",
                "org.apache.hadoop.io.Text:void <clinit>()",
                "org.apache.hadoop.fs.FileStatus:int compareTo(java.lang.Object)",
                "org.apache.hadoop.io.Text:java.lang.String toString()",
                "org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder:org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto$Builder clone()",
                "org.apache.hadoop.security.token.Token:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:int hashCode()",
                "org.apache.hadoop.fs.Path:boolean equals(java.lang.Object)",
                "org.apache.hadoop.conf.Configuration:void set(java.lang.String,java.lang.String)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder:org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto$Builder clone()",
                "org.apache.hadoop.fs.permission.AclStatus:boolean equals(java.lang.Object)",
                "org.apache.hadoop.security.UserGroupInformation$RealUser:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.Path:void <clinit>()",
                "org.apache.hadoop.fs.Path:void <init>(java.lang.String)",
                "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:boolean equals(java.lang.Object)",
                "org.apache.hadoop.metrics2.util.Quantile:int hashCode()",
                "org.apache.hadoop.io.serializer.SerializationFactory:void <init>(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder:org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto$Builder clone()",
                "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.serializer.SerializationFactory:void <clinit>()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RPCTraceInfoProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.Path:int hashCode()",
                "org.apache.hadoop.fs.FileStatus:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto:int hashCode()",
                "org.apache.hadoop.fs.Path:java.lang.String getName()",
                "org.apache.hadoop.security.User:int hashCode()",
                "org.apache.hadoop.io.UTF8:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder:org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$Builder clone()",
                "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered$1:java.util.Iterator iterator()",
                "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.FileUtil:boolean fullyDelete(java.io.File)",
                "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:int hashCode()",
                "org.apache.hadoop.conf.Configuration:void <clinit>()",
                "org.apache.hadoop.io.DataInputBuffer:void <init>()",
                "org.apache.hadoop.io.SequenceFile$Metadata:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolVersionProto:int hashCode()",
                "org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto:int hashCode()",
                "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:boolean hasNext()",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto$Builder:org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureResponseProto$Builder clone()",
                "org.apache.hadoop.fs.Path:void <init>(java.net.URI)",
                "org.apache.hadoop.fs.FileUtil:void unZip(java.io.File,java.io.File)",
                "org.apache.hadoop.conf.Configuration:void <init>()",
                "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:java.util.Iterator iterator()",
                "org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$ProtocolSignatureProto:boolean equals(java.lang.Object)",
                "org.apache.hadoop.security.UserGroupInformation:boolean equals(java.lang.Object)",
                "org.apache.hadoop.fs.Path:void <init>(java.lang.String,java.lang.String)",
                "org.apache.hadoop.io.BytesWritable:int hashCode()",
                "org.apache.hadoop.util.ShutdownHookManager$2:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.security.token.Token:boolean equals(java.lang.Object)",
                "org.apache.hadoop.util.RunJar:void <clinit>()",
                "org.apache.hadoop.util.ApplicationClassLoader:java.net.URL getResource(java.lang.String)",
                "org.apache.hadoop.fs.LocatedFileStatus:int compareTo(java.lang.Object)",
                "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:int hashCode()",
                "org.apache.hadoop.conf.Configuration:int getInt(java.lang.String,int)",
                "org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder:org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto$Builder clone()",
                "org.apache.hadoop.io.BytesWritable:boolean equals(java.lang.Object)",
                "org.apache.hadoop.io.Text:void <init>(java.lang.String)"
            ],
            "CVE": [
                "CVE-2022-25168"
            ],
            "used vul-method": [
                "org.apache.hadoop.fs.FileUtil:void unTar(java.io.File,java.io.File)"
            ],
            "vul-called frequency": 1,
            "related vul root method": {
                "CVE-2022-25168": [
                    "org.apache.hadoop.fs.FileUtil:void unTarUsingTar(java.io.File,java.io.File,boolean)"
                ]
            },
            "CVE-API": {
                "CVE-2022-25168": [
                    "org.apache.hadoop.fs.FileUtil:void unTar(java.io.File,java.io.File)"
                ]
            }
        },
        "org.apache.hadoop:hadoop-mapreduce-client-core:2.6.0": {
            "used-method num": 173,
            "used method": [
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:java.lang.Class getOutputKeyClass()",
                "org.apache.hadoop.mapred.Counters:org.apache.hadoop.mapred.Counters$Counter findCounter(java.lang.String,java.lang.String)",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:java.net.URI[] getCacheArchives()",
                "org.apache.hadoop.mapreduce.TaskAttemptID:int hashCode()",
                "org.apache.hadoop.mapred.JVMId:int hashCode()",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.JobSubmitter$SplitComparator:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.TaskAttemptID:org.apache.hadoop.mapred.TaskAttemptID forName(java.lang.String)",
                "org.apache.hadoop.mapreduce.Counters:void <init>()",
                "org.apache.hadoop.mapreduce.lib.input.FileSplit:void <init>(org.apache.hadoop.fs.Path,long,long,java.lang.String[])",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:void setCacheFiles(java.net.URI[],org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapred.Counters$Counter:java.lang.String getName()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:void setCacheArchives(java.net.URI[],org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.fs.Path[] getLocalCacheArchives()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:org.apache.hadoop.mapreduce.TaskAttemptID getTaskAttemptID()",
                "org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector:void close(org.apache.hadoop.mapreduce.TaskAttemptContext)",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:org.apache.hadoop.mapreduce.Counter getCounter(java.lang.String,java.lang.String)",
                "org.apache.hadoop.mapred.Task$NewCombinerRunner$OutputConverter:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup$FSCounter:long getValue()",
                "org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup:java.util.Iterator iterator()",
                "org.apache.hadoop.mapred.JobConf:void <clinit>()",
                "org.apache.hadoop.mapred.Counters$Group:java.util.Iterator iterator()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:java.lang.Class getOutputKeyClass()",
                "org.apache.hadoop.mapred.Counters:java.util.Collection getGroupNames()",
                "org.apache.hadoop.mapreduce.Job:void <clinit>()",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:java.lang.Class getOutputValueClass()",
                "org.apache.hadoop.mapred.Counters:void incrCounter(java.lang.String,java.lang.String,long)",
                "org.apache.hadoop.mapred.Queue:int hashCode()",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:boolean getSymlink()",
                "org.apache.hadoop.mapreduce.TaskCompletionEvent:int hashCode()",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:org.apache.hadoop.mapreduce.Counter findCounter(java.lang.Enum)",
                "org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:int hashCode()",
                "org.apache.hadoop.mapreduce.JobSubmitter$1:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.Merger$MergeQueue$1:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.mapreduce.TaskAttemptID getTaskAttemptID()",
                "org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter:void close(org.apache.hadoop.mapreduce.TaskAttemptContext)",
                "org.apache.hadoop.mapreduce.QueueState:java.lang.String toString()",
                "org.apache.hadoop.mapred.Counters$Counter:int hashCode()",
                "org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.fs.Path[] getLocalCacheFiles()",
                "org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:void <clinit>()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.mapreduce.Counter getCounter(java.lang.String,java.lang.String)",
                "org.apache.hadoop.mapred.Counters:void incrCounter(java.lang.Enum,long)",
                "org.apache.hadoop.mapreduce.Job:void <init>(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:boolean getSymlink()",
                "org.apache.hadoop.mapreduce.TaskAttemptID:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounterGroup:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapred.SortedRanges$Range:int hashCode()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:java.lang.Object getCurrentValue()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.FileInputFormat:void setInputPaths(org.apache.hadoop.mapred.JobConf,java.lang.String)",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:void addCacheArchive(java.net.URI,org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.JobStatus:java.lang.Object clone()",
                "org.apache.hadoop.mapreduce.counters.GenericCounter:java.lang.String getName()",
                "org.apache.hadoop.mapreduce.Counters:void <clinit>()",
                "org.apache.hadoop.mapreduce.TaskID:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader:boolean nextKeyValue()",
                "org.apache.hadoop.mapreduce.JobID:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.MapTask$NewOutputCollector:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.TaskAttemptID:org.apache.hadoop.mapreduce.TaskAttemptID forName(java.lang.String)",
                "org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter:void close(org.apache.hadoop.mapred.Reporter)",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:org.apache.hadoop.fs.Path[] getLocalCacheArchives(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup$FSCounter:java.lang.String getName()",
                "org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterable:java.util.Iterator iterator()",
                "org.apache.hadoop.mapred.MapTask$SkippingRecordReader:boolean next(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounterGroup:java.util.Iterator iterator()",
                "org.apache.hadoop.mapred.Counters$Counter:long getValue()",
                "org.apache.hadoop.mapreduce.TaskAttemptID:java.lang.String toString()",
                "org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup$FrameworkCounter:java.lang.String getName()",
                "org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$CompressAwarePath:int compareTo(java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:java.lang.Class getOutputKeyClass()",
                "org.apache.hadoop.mapreduce.JobID:int compareTo(java.lang.Object)",
                "org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader:java.lang.Object getCurrentValue()",
                "org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:java.lang.Object next()",
                "org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator:java.lang.Object next()",
                "org.apache.hadoop.mapreduce.counters.AbstractCounter:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:org.apache.hadoop.conf.Configuration getConfiguration()",
                "org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter:void close(org.apache.hadoop.mapreduce.TaskAttemptContext)",
                "org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:java.lang.Long next()",
                "org.apache.hadoop.mapreduce.JobID:int hashCode()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:int getNumReduceTasks()",
                "org.apache.hadoop.mapred.FileOutputFormat:void setOutputPath(org.apache.hadoop.mapred.JobConf,org.apache.hadoop.fs.Path)",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:java.lang.Iterable getValues()",
                "org.apache.hadoop.mapreduce.TaskCompletionEvent:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapred.Counters$Group:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$CompressAwarePath:int hashCode()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:org.apache.hadoop.mapreduce.Counter getCounter(java.lang.Enum)",
                "org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup:int hashCode()",
                "org.apache.hadoop.mapred.Counters:void <init>()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:java.net.URI[] getCacheFiles()",
                "org.apache.hadoop.mapred.Task$CombineValuesIterator:boolean hasNext()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:java.net.URI[] getCacheArchives()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:boolean nextKeyValue()",
                "org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader:java.lang.Object getCurrentKey()",
                "org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader:void initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)",
                "org.apache.hadoop.mapred.MapTask$TrackedRecordReader:boolean next(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.Counters$Counter:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.TaskID:int compareTo(java.lang.Object)",
                "org.apache.hadoop.mapred.MapTask$TrackedRecordReader:java.lang.Object createKey()",
                "org.apache.hadoop.mapred.SortedRanges$Range:int compareTo(java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounterGroup:int hashCode()",
                "org.apache.hadoop.mapred.Task$ValuesIterator:boolean hasNext()",
                "org.apache.hadoop.mapreduce.lib.output.MultipleOutputs:void <init>(org.apache.hadoop.mapreduce.TaskInputOutputContext)",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:java.net.URI[] getCacheArchives(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:org.apache.hadoop.fs.Path[] getLocalCacheFiles()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:java.lang.Class getOutputValueClass()",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:org.apache.hadoop.conf.Configuration getConfiguration()",
                "org.apache.hadoop.mapred.JobConf:org.apache.hadoop.io.RawComparator getOutputValueGroupingComparator()",
                "org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:boolean hasNext()",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:java.net.URI[] getCacheFiles()",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:java.util.Iterator iterator()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:java.lang.Object getCurrentKey()",
                "org.apache.hadoop.mapreduce.TaskID:int hashCode()",
                "org.apache.hadoop.mapred.MapTask$NewOutputCollector:void close(org.apache.hadoop.mapreduce.TaskAttemptContext)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:java.lang.Iterable getGroupNames()",
                "org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup:java.util.Iterator iterator()",
                "org.apache.hadoop.mapreduce.counters.FrameworkCounterGroup$FrameworkCounter:long getValue()",
                "org.apache.hadoop.mapred.Queue:int compareTo(java.lang.Object)",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:org.apache.hadoop.fs.Path[] getLocalCacheFiles(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.TaskAttemptID:int compareTo(java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:org.apache.hadoop.fs.Path[] getLocalCacheFiles()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:java.lang.Class getOutputValueClass()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:int getNumReduceTasks()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:java.net.URI[] getCacheFiles()",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:org.apache.hadoop.fs.Path[] getLocalCacheArchives()",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:void addCacheFile(java.net.URI,org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:org.apache.hadoop.mapreduce.Counter getCounter(java.lang.Enum)",
                "org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:void <clinit>()",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:java.net.URI[] getCacheFiles(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapred.JVMId:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.mapreduce.Counter getCounter(java.lang.Enum)",
                "org.apache.hadoop.mapred.SortedRanges$SkipRangeIterator:boolean hasNext()",
                "org.apache.hadoop.mapred.JobConf:void setMapOutputKeyClass(java.lang.Class)",
                "org.apache.hadoop.mapred.ReduceTask$2:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:org.apache.hadoop.mapreduce.Counter getCounter(java.lang.String,java.lang.String)",
                "org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl:org.apache.hadoop.mapreduce.TaskAttemptID getTaskAttemptID()",
                "org.apache.hadoop.mapred.Task$CombineValuesIterator:java.lang.Object next()",
                "org.apache.hadoop.mapred.Counters$Group:int hashCode()",
                "org.apache.hadoop.mapred.Reporter:void <clinit>()",
                "org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$1:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:int hashCode()",
                "org.apache.hadoop.mapreduce.Mapper:void run(org.apache.hadoop.mapreduce.Mapper$Context)",
                "org.apache.hadoop.mapreduce.counters.GenericCounter:long getValue()",
                "org.apache.hadoop.mapreduce.task.reduce.MapOutput$MapOutputComparator:int compare(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.filecache.DistributedCache:boolean getSymlink(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$CompressAwarePath:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapred.FileInputFormat:void <clinit>()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.mapreduce.InputSplit getInputSplit()",
                "org.apache.hadoop.mapreduce.counters.FileSystemCounterGroup:boolean equals(java.lang.Object)",
                "org.apache.hadoop.mapreduce.counters.AbstractCounter:int hashCode()",
                "org.apache.hadoop.mapred.Counters:org.apache.hadoop.mapred.Counters$Group getGroup(java.lang.String)",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:java.lang.Object getCurrentKey()",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:org.apache.hadoop.mapreduce.Counter findCounter(java.lang.String,java.lang.String)",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:org.apache.hadoop.conf.Configuration getConfiguration()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:boolean nextKey()",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:int getNumReduceTasks()",
                "org.apache.hadoop.mapred.JobConf:void <init>(org.apache.hadoop.conf.Configuration)",
                "org.apache.hadoop.mapred.FileSplit:void <init>(org.apache.hadoop.fs.Path,long,long,java.lang.String[])",
                "org.apache.hadoop.mapred.Counters:void <clinit>()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:org.apache.hadoop.fs.Path[] getLocalCacheArchives()",
                "org.apache.hadoop.mapreduce.lib.output.MultipleOutputs$RecordWriterWithCounter:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapreduce.lib.output.SequenceFileAsBinaryOutputFormat$1:void write(java.lang.Object,java.lang.Object)",
                "org.apache.hadoop.mapred.JobConf:org.apache.hadoop.io.RawComparator getOutputKeyComparator()",
                "org.apache.hadoop.mapred.MapTask$TrackedRecordReader:java.lang.Object createValue()",
                "org.apache.hadoop.mapreduce.counters.AbstractCounters:org.apache.hadoop.mapreduce.counters.CounterGroupBase getGroup(java.lang.String)",
                "org.apache.hadoop.mapreduce.task.JobContextImpl:java.net.URI[] getCacheArchives()",
                "org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context:boolean getSymlink()"
            ],
            "CVE": [
                "CVE-2015-1776"
            ],
            "used vul-method": [
                "org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:java.lang.Object next()",
                "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:boolean nextKey()"
            ],
            "vul-called frequency": 58,
            "related vul root method": {
                "CVE-2015-1776": [
                    "org.apache.hadoop.mapreduce.CryptoUtils:byte[] getEncryptionKey()"
                ]
            },
            "CVE-API": {
                "CVE-2015-1776": [
                    "org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterator:java.lang.Object next()",
                    "org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context:boolean nextKey()"
                ]
            }
        }
    }
}